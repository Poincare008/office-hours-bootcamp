{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Gradient Boosting - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll learn how to use both Adaboost and Gradient Boosting classifiers from scikit-learn!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Use AdaBoost to make predictions on a dataset \n",
    "- Use Gradient Boosting to make predictions on a dataset \n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In this lab, we'll learn how to use boosting algorithms to make classifications on the [Pima Indians Dataset](http://ftp.ics.uci.edu/pub/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.names). You will find the data stored in the file `'pima-indians-diabetes.csv'`. Our goal is to use boosting algorithms to determine whether a person has diabetes. Let's get started!\n",
    "\n",
    "We'll begin by importing everything we need for this lab. Run cell below:"
   ],
   "id": "f6010b6232ce213a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report"
   ],
   "id": "407802d51d3a9de5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, use Pandas to import the data stored in `'pima-indians-diabetes.csv'` and store it in a DataFrame. Print the first five rows to inspect the data we've imported and ensure everything loaded correctly. ",
   "id": "af54724b2cab5afb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import the data\n",
    "df = pd.read_csv('../data/pima-indians-diabetes.csv')\n",
    "\n",
    "# Print the first five rows\n",
    "df.head()"
   ],
   "id": "c7f947d455dc175a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cleaning, exploration, and preprocessing\n",
    "\n",
    "The target we're trying to predict is the `'Outcome'` column. A `1` denotes a patient with diabetes. \n",
    "\n",
    "By now, you're quite familiar with exploring and preprocessing a dataset.  \n",
    "\n",
    "In the following cells:\n",
    "\n",
    "* Check for missing values and deal with them as you see fit (if any exist) \n",
    "* Count the number of patients with and without diabetes in this dataset \n",
    "* Store the target column in a separate variable and remove it from the dataset\n",
    "* Split the dataset into training and test sets, with a `test_size` of 0.25 and a `random_state` of 42"
   ],
   "id": "ff3dad8411eda89a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ],
   "id": "931fd19319a3c20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Number of patients with and without diabetes\n",
    "print(df['Outcome'].value_counts())"
   ],
   "id": "10b247ed68bcda26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "target = df['Outcome']\n",
    "df = df.drop(columns=['Outcome'])"
   ],
   "id": "48b3b15131f5c659"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.25, random_state=42)"
   ],
   "id": "af6cb430d7786226"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train the models\n",
    "\n",
    "Now that we've explored the dataset, we're ready to fit some models!\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Instantiate an `AdaBoostClassifier` (set the `random_state` for 42)\n",
    "* Instantiate a `GradientBoostingClassifer` (set the `random_state` for 42) "
   ],
   "id": "2355d094f5ec9310"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Instantiate an AdaBoostClassifier\n",
    "adaboost_clf = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Instantiate an GradientBoostingClassifier\n",
    "gbt_clf = GradientBoostingClassifier(random_state=42)"
   ],
   "id": "5a589a2dd8e1ba8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, fit the training data to both the classifiers: ",
   "id": "ebe5a0063e76dc84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fit AdaBoostClassifier\n",
    "adaboost_clf.fit(X_train, y_train)"
   ],
   "id": "1c2d6866ec9cf297"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fit GradientBoostingClassifier\n",
    "gbt_clf.fit(X_train, y_train)"
   ],
   "id": "35f4160febaacb20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's use these models to predict labels on both the training and test sets: ",
   "id": "eb969b38c2066a80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# AdaBoost model predictions\n",
    "adaboost_train_preds = adaboost_clf.predict(X_train)\n",
    "adaboost_test_preds = adaboost_clf.predict(X_test)\n",
    "\n",
    "# GradientBoosting model predictions\n",
    "gbt_clf_train_preds = gbt_clf.predict(X_train)\n",
    "gbt_clf_test_preds = gbt_clf.predict(X_test)"
   ],
   "id": "aa2cbd30c080a5c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, complete the following function and use it to calculate the accuracy and f1-score for each model: ",
   "id": "17dab612cfff6003"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def display_acc_and_f1_score(true, preds, model_name):\n",
    "    acc = accuracy_score(true, preds)\n",
    "    f1 = f1_score(true, preds)\n",
    "    print(\"Model: {}\".format(model_name))\n",
    "    print(\"Accuracy: {}\".format(acc))\n",
    "    print(\"F1-Score: {}\".format(f1))\n",
    "    \n",
    "print(\"Training Metrics\")\n",
    "display_acc_and_f1_score(y_train, adaboost_train_preds, model_name='AdaBoost')\n",
    "print(\"\")\n",
    "display_acc_and_f1_score(y_train, gbt_clf_train_preds, model_name='Gradient Boosted Trees')\n",
    "print(\"\")\n",
    "print(\"Testing Metrics\")\n",
    "display_acc_and_f1_score(y_test, adaboost_test_preds, model_name='AdaBoost')\n",
    "print(\"\")\n",
    "display_acc_and_f1_score(y_test, gbt_clf_test_preds, model_name='Gradient Boosted Trees')"
   ],
   "id": "b102e0e58dddb7e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's go one step further and create a confusion matrix and classification report for each. Do so in the cell below: ",
   "id": "9079c1ab5f28c412"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "adaboost_confusion_matrix = confusion_matrix(y_test, adaboost_test_preds)\n",
    "adaboost_confusion_matrix"
   ],
   "id": "90c0678fdb897aaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gbt_confusion_matrix = confusion_matrix(y_test, gbt_clf_test_preds)\n",
    "gbt_confusion_matrix"
   ],
   "id": "b0871df60ac1cb6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "adaboost_classification_report = classification_report(y_test, adaboost_test_preds)\n",
    "print(adaboost_classification_report)"
   ],
   "id": "b5c43b9d0baeee61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gbt_classification_report = classification_report(y_test, gbt_clf_test_preds)\n",
    "print(gbt_classification_report)"
   ],
   "id": "8cd6004d04209157"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**_Question:_** How did the models perform? Interpret the evaluation metrics above to answer this question.\n",
    "\n",
    "Write your answer below this line:\n",
    "_______________________________________________________________________________________________________________________________\n",
    "\n",
    " \n",
    " \n",
    "As a final performance check, let's calculate the 5-fold cross-validated score for each model! \n",
    "\n",
    "Recall that to compute the cross-validation score, we need to pass in:\n",
    "\n",
    "* A classifier\n",
    "* All training data\n",
    "* All labels\n",
    "* The number of folds we want in our cross-validation score  \n",
    "\n",
    "Since we're computing cross-validation score, we'll want to pass in the entire dataset, as well as all of the labels. \n",
    "\n",
    "In the cells below, compute the mean cross validation score for each model. "
   ],
   "id": "2b6c6a02e2f1b71f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('Mean Adaboost Cross-Val Score (k=5):')\n",
    "ada_cv_scores = cross_val_score(adaboost_clf, df, target, cv=5)\n",
    "print(np.mean(ada_cv_scores))\n",
    "# Expected Output: 0.7631270690094218"
   ],
   "id": "c9cd2395587db609"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('Mean GBT Cross-Val Score (k=5):')\n",
    "gbt_cv_scores = cross_val_score(gbt_clf, df, target, cv=5)\n",
    "print(np.mean(gbt_cv_scores))\n",
    "# Expected Output: 0.7591715474068416"
   ],
   "id": "5d585ddba8c63a86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These models didn't do poorly, but we could probably do a bit better by tuning some of the important parameters such as the **_Learning Rate_**. \n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, we learned how to use scikit-learn's implementations of popular boosting algorithms such as AdaBoost and Gradient Boosted Trees to make classification predictions on a real-world dataset!"
   ],
   "id": "1606b2f09aaf7e3b"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
